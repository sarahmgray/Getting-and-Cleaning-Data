getwd()
getwd()
q()
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileUrl, destfile="./data/microdata.csv", method="curl")
if(!file.exists("data")) {
dir.create("data")
}
download.file(fileUrl, destfile="./data/microdata.csv", method="curl")
download.file(fileUrl, destfile="./data/microdata.csv")
microData <- read.table("./data/microdata.csv", sep=",", header=TRUE)
sum(!is.na(microData$VAL[microData$VAL==24]))
microdata$FES
microData$FES
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileUrl, destfile="./data/nga.xlsx", method="curl")
download.file(fileUrl, destfile="./data/nga.xlsx")
dateDownloaded <- date()
library(xlsx)
download.file(fileUrl, destfile="./data/nga.xlsx", mode='wb')
library(xlsx)
install.packages("rJava")
install.packages("xlsxjars")
install.packages("xlsx")
library(rJava)
library(xlsxjars)
library(xlsx)
colIndex <- 7:15
rowIndex <- 18:23
dat <- read.xlsx(".data/nga.xlsx", sheetIndex=1, header=TRUE, colIndex=colIndex, rowIndex=rowIndex)
dat <- read.xlsx(".data/nga.xlsx", sheetIndex=1, header=TRUE, colIndex=colIndex, rowIndex=rowIndex)
dat <- read.xlsx("./data/nga.xlsx", sheetIndex=1, header=TRUE, colIndex=colIndex, rowIndex=rowIndex)
sum(dat$ZIP*dat$Ext, na.rm=T)
sum(dat$Zip*dat$Ext,na.rm=T)
library(xlsx)
fileURL <-"https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileURL, destfile="./data/nga.xlsx", method="curl")
download.file(fileUrl, destfile="./data/nga.xlsx", method="curl")
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileUrl, destfile="./data/nga.xlsx", method="curl")
if(!file.exists("data")) {
dir.create("data")
}
download.file(fileUrl, destfile="./data/nga.xlsx", method="curl")
download.file(fileUrl, destfile="./data/nga.xlsx")
dateDownloaded <- date()
library(xlsx)
colIndex <- 7:15
rowIndex <- 18:23
dat <- read.xlsx("./data/nga.xlsx", sheetIndex=1, header=TRUE, colIndex=colIndex, rowIndex=rowIndex)
fileUrl <- "http://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse(fileUrl, useInternal=TRUE)
library(xmlTreeParse)
library(XML)
doc <- xmlTreeParse(fileUrl, useInternal=TRUE)
rootNode <- xmlRoot(doc)
sum(xpathSApply(rootNode, "//zipcode", xmlValue)==21231)
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv "
fileUrl <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileUrl, destfile="./data/microdata3.csv", method="curl")
download.file(fileUrl, destfile="./data/microdata3.csv")
DT <- fread("./data/microdata3.csv")
library(data.table)
DT <- fread("./data/microdata3.csv")
file.info("./data/microdata3.csv")$size
system.time(DT[,mean(pwgtp15),by=SEX])
system.time(mean(DT[DT$SEX==1,]$pwgtp15))+system.time(mean(DT[DT$SEX==2,]$pwgtp15))
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean))
library(sqldf)
extsts(Rtools)
install.packages("sql")
install.packages(c("sqldf", "sqlutils", "RJDBC", "RODBC"))
exists(Rtools)
install.packages("Rtools")
?RTools
??Rtools
?Rtools
install.packages(httpuv)
install.packages("httpuv")
con = url("http://biostat.jhsph.edu/~jleek/contact.html")
htmlCode = readLines(con)
close(con)
htmlCode[1:9]
?nchar
nchar(htmlCode[10])
nchar(htmlCode[20])
nchar(htmlCode[30])
nchar(htmlCode[100])
read.fwf(https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for)
read.fwf(https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for, header=TRUE)
read.fwf(https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for, widths=61, header=false, sep" ", skip=4)
req <- GET("https://api.github.com/repos", config(token = github_token))
> stop_for_status(req)
> content(req)
> BROWSE("https://api.github.com/users/jtleek/repos",authenticate("Access Token","x-oauth-basic","basic"))
req <- GET("https://api.github.com/repos", config(token = github_token))
> stop_for_status(req)
> content(req)
> BROWSE("https://api.github.com/users/jtleek/repos",authenticate("Access Token","x-oauth-basic","basic"))
data <- read.csv("https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for", header = TRUE)
> file_name <- "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
> df <- read.fwf(file=file_name,widths=c(-1,9,-5,4,4,-5,4,4,-5,4,4,-5,4,4), skip=4)
> sum(df[, 4])
req <- GET("https://api.github.com/repos", config(token = github_token))
stop_for_status(req)
content(req)
BROWSE("https://api.github.com/users/jtleek/repos",authenticate("Access Token","x-oauth-basic","basic"))
data <- read.csv("https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for", header = TRUE)
file_name <- "https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for"
df <- read.fwf(file=file_name,widths=c(-1,9,-5,4,4,-5,4,4,-5,4,4,-5,4,4), skip=4)
sum(df[, 4])
setwd("C:/Users/SA-Sgray/Getting-and-Cleaning-Data-Course-Project")
##Single R script that fulfills requirements associated with the Course Project
##for Getting and Cleaning Data
# Step 1: Merge the training and the test sets to create one data set.
### loadData(type) - load data and merge ydata and measuerment values for test
### or train - 'type': indicating test or train data
setwd("C:/Users/SA-Sgray/Getting-and-Cleaning-Data-Course-Project")
loadData <- function(type) {
# load data
folder <- "UCI HAR Dataset/"
filetype <- ".txt"
subjects <- read.table(paste(folder, type, "/subject_", type, filetype,
sep = ""))
y <- read.table(paste(folder, type, "/y_", type, filetype, sep = ""))
FeaturesName <- read.table(paste(folder, "features", filetype, sep = ""))
x <- read.table(paste(folder, type, "/X_", type, filetype, sep = ""))
# Put header of the feature value with features name
names(x) <- FeaturesName[, 2]
# Put header of the activity value and subject with proper name
colnames(subjects) <- "subject"
colnames(y) <- "activity"
# Combine the y ,subject and x together
df <- cbind(cbind(subjects, y), x)
return(df)
}
##setwd("./SA-Sgray/Getting-and-Cleaning-Data-Course-Project/")
test <- loadData("test")
train <- loadData("train")
FullData <- rbind(train, test)
## Step 3: Uses descriptive activity names to name the activities in the data set
# Replace the activity with corrsponding activity name
activityNames <- read.table("UCI HAR Dataset/activity_labels.txt")
FullData$activity <- sapply(FullData$activity, function(x) activityNames[x, 2])
## Step 2: Extracts only the measurements on the mean and standard deviation for
## each measurement.
# Keep features that represents for mean and standard deviation (std)
featuresNames <- read.table("UCI HAR Dataset/features.txt")
colnames(featuresNames) <- c("fId", "featurename")
keepFeatures = subset(featuresNames, grepl("mean|std", featurename))[, 2]
## Step4: Appropriately labels the data set with descriptive variable names.
# Select the features we want
preData <- subset(FullData, select = c(cbind("subject", "activity"),
as.character(keepFeatures)))
## Step5: From the data set in step 4, creates a second, independent tidy data
## set with the average of each variable for each activity and each subject.
# Use reshape library to create a dataframe with identity vaiable 'subject'
# and 'activity'
library(reshape)
tidyMelted <- melt(preData, id = c("subject", "activity"))
tidy <- cast(tidyMelted, subject + activity ~ variable, mean)
## dim(tidy)  # See the row and column number
## summary(tidy)
write.table(tidy, "tidy.txt")
q()
##Single R script that fulfills requirements associated with the Course Project
##for Getting and Cleaning Data
# Step 1: Merge the training and the test sets to create one data set.
### loadData(type) - load data and merge ydata and measuerment values for test
### or train - 'type': indicating test or train data
setwd("C:/Users/SA-Sgray/Getting-and-Cleaning-Data-Course-Project")
loadData <- function(type) {
# load data
folder <- "UCI HAR Dataset/"
filetype <- ".txt"
subjects <- read.table(paste(folder, type, "/subject_", type, filetype,
sep = ""))
y <- read.table(paste(folder, type, "/y_", type, filetype, sep = ""))
FeaturesName <- read.table(paste(folder, "features", filetype, sep = ""))
x <- read.table(paste(folder, type, "/X_", type, filetype, sep = ""))
# Put header of the feature value with features name
names(x) <- FeaturesName[, 2]
# Put header of the activity value and subject with proper name
colnames(subjects) <- "subject"
colnames(y) <- "activity"
# Combine the y ,subject and x together
df <- cbind(cbind(subjects, y), x)
return(df)
}
##setwd("./SA-Sgray/Getting-and-Cleaning-Data-Course-Project/")
test <- loadData("test")
train <- loadData("train")
FullData <- rbind(train, test)
## Step 3: Uses descriptive activity names to name the activities in the data set
# Replace the activity with corrsponding activity name
activityNames <- read.table("UCI HAR Dataset/activity_labels.txt")
FullData$activity <- sapply(FullData$activity, function(x) activityNames[x, 2])
## Step 2: Extracts only the measurements on the mean and standard deviation for
## each measurement.
# Keep features that represents for mean and standard deviation (std)
featuresNames <- read.table("UCI HAR Dataset/features.txt")
colnames(featuresNames) <- c("fId", "featurename")
keepFeatures = subset(featuresNames, grepl("mean|std", featurename))[, 2]
## Step4: Appropriately labels the data set with descriptive variable names.
# Select the features we want
preData <- subset(FullData, select = c(cbind("subject", "activity"),
as.character(keepFeatures)))
## Step5: From the data set in step 4, creates a second, independent tidy data
## set with the average of each variable for each activity and each subject.
# Use reshape library to create a dataframe with identity vaiable 'subject'
# and 'activity'
library(reshape)
tidyMelted <- melt(preData, id = c("subject", "activity"))
tidy <- cast(tidyMelted, subject + activity ~ variable, mean)
## dim(tidy)  # See the row and column number
## summary(tidy)
write.table(tidy, "tidy.txt")
view(tidy)
View(tidy)
setwd("C:/Users/SA-Sgray/Getting-and-Cleaning-Data")
test <- loadData("test")
train <- loadData("train")
FullData <- rbind(train, test)
loadData <- function(type) {
# load data
folder <- "UCI HAR Dataset/"
filetype <- ".txt"
subjects <- read.table(paste(folder, type, "/subject_", type, filetype,
sep = ""))
y <- read.table(paste(folder, type, "/y_", type, filetype, sep = ""))
FeaturesName <- read.table(paste(folder, "features", filetype, sep = ""))
x <- read.table(paste(folder, type, "/X_", type, filetype, sep = ""))
# Put header of the feature value with features name
names(x) <- FeaturesName[, 2]
# Put header of the activity value and subject with proper name
colnames(subjects) <- "subject"
colnames(y) <- "activity"
# Combine the y ,subject and x together
df <- cbind(cbind(subjects, y), x)
return(df)
}
test <- loadData("test")
train <- loadData("train")
FullData <- rbind(train, test)
head(FullData)
View(preData)
## Step 3: Uses descriptive activity names to name the activities in the data set
# Replace the activity with corrsponding activity name
activityNames <- read.table("UCI HAR Dataset/activity_labels.txt")
FullData$activity <- sapply(FullData$activity, function(x) activityNames[x, 2])
## Step 2: Extracts only the measurements on the mean and standard deviation for
## each measurement.
# Keep features that represents for mean and standard deviation (std)
featuresNames <- read.table("UCI HAR Dataset/features.txt")
colnames(featuresNames) <- c("fId", "featurename")
keepFeatures = subset(featuresNames, grepl("mean|std", featurename))[, 2]
## Step4: Appropriately labels the data set with descriptive variable names.
# Select the features we want
preData <- subset(FullData, select = c(cbind("subject", "activity"),
as.character(keepFeatures)))
## Step5: From the data set in step 4, creates a second, independent tidy data
## set with the average of each variable for each activity and each subject.
# Use reshape library to create a dataframe with identity vaiable 'subject'
# and 'activity'
library(reshape)
tidyMelted <- melt(preData, id = c("subject", "activity"))
tidy <- cast(tidyMelted, subject + activity ~ variable, mean)
## dim(tidy)  # See the row and column number
## summary(tidy)
write.table(tidy, "tidy.txt")
View(preData)
